{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 1. Computing RDMs\n\nAs we did for classification, we will use an MEG dataset of humans performing a visual categorization task. Briefly, participants saw a list of 92 images. Effectively, these images are faces, not faces, human, not human, artificial, etc. For more information, consult [MNE's documentation](https://mne.tools/1.8/auto_examples/decoding/decoding_rsa_sgskip.html) or the [original paper](https://dx.doi.org/10.1038/nn.3635). Our goal here will be to create a neural RDM and a hypothesis RDM based on the neural data and the corresponding categories and see if we find some similarity between the two.\n\nFirst, we will have to load the data---To do this, we use MNE's sample code. Be aware that this will download roughly 6GB of data, which may take a while. As we did in the classification example, we will again be using only gradiometers (for convenience).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom pandas import read_csv\n\nimport mne\nfrom mne.datasets import visual_92_categories\nfrom mne.io import concatenate_raws, read_raw_fif\n\nprint(__doc__)\n\ndata_path = visual_92_categories.data_path()\n\n# Define stimulus - trigger mapping\nfname = data_path / \"visual_stimuli.csv\"\nconds = read_csv(fname)\nprint(conds.head(5))\n\nmax_trigger = 92\nconds = conds[:max_trigger]  # take only the first 24 rows\n\nconditions = []\nfor c in conds.values:\n    cond_tags = list(c[:2])\n    cond_tags += [\n        (\"not-\" if i == 0 else \"\") + conds.columns[k] for k, i in enumerate(c[2:], 2)\n    ]\n    conditions.append(\"/\".join(map(str, cond_tags)))\nprint(conditions[:10])\n\nevent_id = dict(zip(conditions, conds.trigger + 1))\nevent_id[\"0/human bodypart/human/not-face/animal/natural\"]\n\nn_runs = 4  # 4 for full data (use less to speed up computations)\nfnames = [data_path / f\"sample_subject_{b}_tsss_mc.fif\" for b in range(n_runs)]\nraws = [\n    read_raw_fif(fname, verbose=\"error\", on_split_missing=\"ignore\") for fname in fnames\n]  # ignore filename warnings\nraw = concatenate_raws(raws)\n\nevents = mne.find_events(raw, min_duration=0.002)\n\nevents = events[events[:, 2] <= max_trigger]\n\npicks = mne.pick_types(raw.info, meg=True)\nepochs = mne.Epochs(\n    raw,\n    events=events,\n    event_id=event_id,\n    baseline=None,\n    picks='grad',\n    tmin=-0.1,\n    tmax=0.500,\n    preload=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we can compute RDMs, we would like to also get some data to base our hypothesis RDMs on. Specifically, we will use the epoch categories (human/nonhuman/natural/artificial, bodypart/face/inanimate, human/not-human, face/not-face, natural/not-natural) to build features vectors of our images, like so:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# generate some vectors for conditions based on labels\ncondition_types = {}\n\nfor i, cond in enumerate(conditions):\n    items = '/'.join(cond.split(' ')).split('/')[1:]\n    \n    for j, item in enumerate(items):\n        if j not in condition_types:\n            condition_types[j] = []\n        \n        if item not in condition_types[j]:\n            condition_types[j].append(item)\n\nN = np.array([len(condition_types[i]) for i in range(len(condition_types))]).sum()\ncondition_vecs = np.zeros((len(conditions), N))\n\nfor i, cond in enumerate(conditions):\n    items = '/'.join(cond.split(' ')).split('/')[1:]\n    \n    indx_i = 0\n    for j, item in enumerate(items):\n        indx_j = np.where(np.array(condition_types[j]) == item)[0]\n        condition_vecs[i,indx_i+indx_j[0]] = 1\n        indx_i += len(condition_types[j])\n\n# generate vectors for trials based on condition labels\nepochs, indc = epochs.equalize_event_counts()\n\nX = epochs.get_data()\ny = np.zeros((X.shape[0], N, X.shape[-1]))\nL = np.zeros((X.shape[0],))\n\nfor i, event in enumerate(epochs.events):\n    y[i,:,:] = condition_vecs[event[2] - 1,:,None]\n    L[i] = event[2] - 1\n\n# group by condition\nunq, counts = np.unique(L, return_counts = True)\nX_g = np.zeros((counts[0], len(unq), X.shape[1], X.shape[2]))\ny_g = np.zeros((counts[0], len(unq), y.shape[1], y.shape[2]))\n\nfor i, unq_i in enumerate(unq):\n    indc = np.where(L == unq_i)[0]\n    X_g[:,i] = X[indc]\n    y_g[:,i] = y[indc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our data, let's move everything to torch. Note that, by default, this will try to look for a GPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\n# convert data\ndevice = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\nX_g, y_g = torch.from_numpy(X_g).to(torch.float32).to(device), torch.from_numpy(y_g).to(torch.float32).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the setup out of the way, let's create a quick pipeline to compute our neural and hypothesis RDMs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mvpy.estimators import Scaler, RSA\nfrom mvpy.math import *\nfrom sklearn.pipeline import make_pipeline\n\n# compute the neural RDM with the following pipeline:\n#   1. Scale the data (zero mean and unit variance)\n#   2. Compute the neural RDM using the Pearson correlation as our similarity measure\nn_rsa = make_pipeline(Scaler().to_torch(),\n                      RSA(estimator = pearsonr, \n                          verbose = True, \n                          n_jobs = None).to_torch())\nn_rsa.fit(X_g.mean(0))\nn_rsa.transform(X_g.mean(0))\n\n# compute the hypothesis RDM; pipeline is same as neural RDM\nh_rsa = make_pipeline(Scaler().to_torch(),\n                         RSA(estimator = pearsonr, \n                             verbose = True, \n                             n_jobs = None).to_torch())\nh_rsa.fit(y_g.mean(0))\nh_rsa.transform(y_g.mean(0))\n\n# Now, let's compare the two RDMs we have so far by computing their spearman correlations\nrho_pr = spearmanr(n_rsa[1].rdm_.T, h_rsa[1].rdm_.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With our result in hand, we may find ourselves wanting to look at the results and some of the raw RSMs we computed. Let's do that now:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows = 1, ncols = 3, figsize = (9.0, 4.0))\nmax_tp = rho_pr.argmax().cpu().numpy()\n\nt = np.arange(-0.1, 0.5+1e-3, 1e-3)     # time points\nax[0].plot(t, 0*t, color = 'red')       # plot chance level\nax[0].plot(t, rho_pr.cpu().numpy())     # plot spearman correlation\nax[0].set_xlabel(r'Time ($s$)')\nax[0].set_ylabel(r'Spearman $\\rho$')\n\nax[1].imshow(n_rsa[1].full_rdm().cpu().numpy()[:,:,max_tp], vmin = -1, vmax = 1, cmap = 'RdBu_r')   # obtain and plot the full neural RDM from our class\nax[1].set_title(fr'nRDM at $t={np.round(max_tp*1e-3 - 0.1, 3)}s$')\nax[1].set_ylabel('Categories')\nax[1].set_xlabel('Categories')\n\nax[2].imshow(h_rsa[1].full_rdm().cpu().numpy()[:,:,max_tp], vmin = -1, vmax = 1, cmap = 'RdBu_r')   # obtain and plot the full hypothesis RDM from our class\nax[2].set_title(fr'hRDM at $t={np.round(max_tp*1e-3 - 0.1, 3)}s$')\nax[2].set_ylabel('Categories')\nax[2].set_xlabel('Categories')\n\nplt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}